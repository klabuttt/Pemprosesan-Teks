{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Y8b3t7bf48fF"
      ],
      "authorship_tag": "ABX9TyNPERw43RUTzxC1TMY5Lm25",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/klabuttt/Pemprosesan-Teks/blob/main/Klasifikasi_Teks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes TF-IDF"
      ],
      "metadata": {
        "id": "Y8b3t7bf48fF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Sastrawi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn6ONLloH4Fv",
        "outputId": "97c07053-7bd5-427b-b475-b0b5ce1b43cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-O8GYMMw45IB",
        "outputId": "b7bc98c3-139e-41be-d172-a91384b3f005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 74.00%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Negatif       0.95      0.73      0.83        26\n",
            "      Netral       0.00      0.00      0.00         6\n",
            "     Positif       0.62      1.00      0.77        18\n",
            "\n",
            "    accuracy                           0.74        50\n",
            "   macro avg       0.52      0.58      0.53        50\n",
            "weighted avg       0.72      0.74      0.71        50\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "# Contoh dataset (ganti dengan dataset Anda sendiri)\n",
        "data = pd.read_csv('Data Manual.csv')\n",
        "\n",
        "# Membuat DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Clean column names by stripping whitespace\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Initialize stop word remover\n",
        "factory = StopWordRemoverFactory()\n",
        "stopword_remover = factory.create_stop_word_remover()\n",
        "# Corrected line: Get stop words from the factory, not the remover object\n",
        "indonesian_stop_words = factory.get_stop_words()\n",
        "\n",
        "# Memisahkan data menjadi fitur (X) dan target (y)\n",
        "X = df['Content']\n",
        "y = df['Label']\n",
        "\n",
        "# Membagi data menjadi data latih dan data uji\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Menggunakan TfidfVectorizer untuk mengubah teks menjadi fitur numerik\n",
        "# Mengubah teks menjadi representasi numerik menggunakan TF-IDF\n",
        "tfidf = TfidfVectorizer(stop_words=indonesian_stop_words)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "# Membuat dan melatih model Naive Bayes\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Memprediksi label untuk data uji\n",
        "y_pred = nb.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluasi model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Menampilkan laporan klasifikasi\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT"
      ],
      "metadata": {
        "id": "GAKwXAKQJs_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLNYiptkIJmg",
        "outputId": "adc59cc7-ad7b-4d6e-cd38-3277e7191d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Dataset contoh\n",
        "data = pd.read_csv('Data Manual.csv')\n",
        "\n",
        "# Clean column names by stripping whitespace\n",
        "data.columns = data.columns.str.strip()\n",
        "\n",
        "# Membagi data menjadi train dan test\n",
        "texts = data['Content']\n",
        "labels_str = data['Label'] # Store original string labels\n",
        "\n",
        "# Encode string labels to numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels_str)\n",
        "\n",
        "X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(texts, labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Get the number of unique labels after encoding\n",
        "num_unique_labels = len(label_encoder.classes_)\n",
        "\n",
        "# Memuat tokenizer dan model pre-trained BERT untuk klasifikasi\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_unique_labels)\n",
        "\n",
        "# Persiapkan dataset untuk BERT\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts.tolist() # Convert pandas Series to list for consistent indexing\n",
        "        self.labels = labels.tolist() # Convert numpy array to list\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item]) # Ensure text is string\n",
        "        label = self.labels[item]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Membuat dataset untuk train dan test\n",
        "MAX_LEN = 32  # panjang maksimal token\n",
        "train_dataset = TextDataset(X_train, y_train_encoded, tokenizer, MAX_LEN)\n",
        "test_dataset = TextDataset(X_test, y_test_encoded, tokenizer, MAX_LEN)\n",
        "\n",
        "# Membuat DataLoader untuk batch training dan testing\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8)\n",
        "\n",
        "# Fungsi untuk melatih model\n",
        "def train_model(model, train_dataloader, optimizer, device):\n",
        "    model = model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return total_loss / len(train_dataloader)\n",
        "\n",
        "# Fungsi untuk mengevaluasi model\n",
        "def eval_model(model, test_dataloader, device):\n",
        "    model = model.eval()\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "    return accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Setup device (GPU/CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=2e-5)\n",
        "\n",
        "# Training model\n",
        "for epoch in range(3):  # Train for 3 epochs\n",
        "    print(f'Epoch {epoch + 1}')\n",
        "    train_loss = train_model(model, train_dataloader, optimizer, device)\n",
        "    print(f'Training loss: {train_loss}')\n",
        "\n",
        "    # Evaluating model\n",
        "    accuracy = eval_model(model, test_dataloader, device)\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SJTo5McJxhc",
        "outputId": "4f1d53bb-0193-418d-d553-32f16c901525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Training loss: 0.9376243090629578\n",
            "Accuracy: 82.00%\n",
            "\n",
            "Epoch 2\n",
            "Training loss: 0.7320686423778534\n",
            "Accuracy: 84.00%\n",
            "\n",
            "Epoch 3\n",
            "Training loss: 0.5727986919879914\n",
            "Accuracy: 82.00%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doc2Vec"
      ],
      "metadata": {
        "id": "liET6lEBOs8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.naive_bayes import GaussianNB # Changed from MultinomialNB\n",
        "\n",
        "# Dataset contoh\n",
        "data = pd.read_csv('Data Manual.csv')\n",
        "\n",
        "# Clean column names by stripping whitespace\n",
        "data.columns = data.columns.str.strip()\n",
        "\n",
        "# Membuat DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Memisahkan data menjadi fitur (X) dan target (y)\n",
        "X = df['Content']\n",
        "y = df['Label']\n",
        "\n",
        "# Membagi data menjadi data latih dan data uji\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenisasi dan menandai dokumen\n",
        "def tag_documents(texts):\n",
        "    return [TaggedDocument(words=text.split(), tags=[str(i)]) for i, text in enumerate(texts)]\n",
        "\n",
        "# Menandai dokumen latih dan uji\n",
        "train_documents = tag_documents(X_train)\n",
        "test_documents = tag_documents(X_test)\n",
        "\n",
        "# Melatih model Doc2Vec\n",
        "model = Doc2Vec(vector_size=20, window=2, min_count=1, workers=4, epochs=100)\n",
        "model.build_vocab(train_documents)\n",
        "model.train(train_documents, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "# Mengonversi dokumen ke vektor menggunakan model Doc2Vec\n",
        "X_train_vectors = [model.infer_vector(doc.words) for doc in train_documents]\n",
        "X_test_vectors = [model.infer_vector(doc.words) for doc in test_documents]\n",
        "\n",
        "# Melatih model Naive Bayes\n",
        "nb_classifier = GaussianNB() # Changed from MultinomialNB\n",
        "nb_classifier.fit(X_train_vectors, y_train)\n",
        "\n",
        "# Prediksi dengan data uji\n",
        "y_pred = nb_classifier.predict(X_test_vectors)\n",
        "\n",
        "# Evaluasi model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwga_di0OhsP",
        "outputId": "212169eb-821c-4e01-a689-659b7a53380c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 44.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BoW"
      ],
      "metadata": {
        "id": "azK05ou-PtbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z258MKGsPsbd",
        "outputId": "d3a29bb5-b3eb-489c-b66b-6749ceb079fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "# Dataset contoh\n",
        "data = pd.read_csv('Data Manual.csv')\n",
        "\n",
        "# Clean column names by stripping whitespace\n",
        "data.columns = data.columns.str.strip()\n",
        "\n",
        "# Membuat DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Memisahkan data menjadi fitur (X) dan target (y)\n",
        "X = df['Content']\n",
        "y = df['Label']\n",
        "\n",
        "# Membagi data menjadi data latih dan data uji\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Sastrawi StopWordRemoverFactory to get Indonesian stop words\n",
        "factory = StopWordRemoverFactory()\n",
        "indonesian_stop_words = factory.get_stop_words()\n",
        "\n",
        "# Menggunakan CountVectorizer untuk mengonversi teks menjadi representasi BoW\n",
        "# Pass the list of Indonesian stop words\n",
        "vectorizer = CountVectorizer(stop_words=indonesian_stop_words)\n",
        "\n",
        "# Fit dan transform data latih menjadi vektor BoW\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "# Membuat dan melatih model Naive Bayes\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_bow, y_train)\n",
        "\n",
        "# Prediksi dengan data uji\n",
        "y_pred = nb_classifier.predict(X_test_bow)\n",
        "\n",
        "# Evaluasi model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Menampilkan laporan klasifikasi\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujDeVRoWPwpg",
        "outputId": "f7b066e1-3d73-4e1f-b997-aa72f1d9f358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 76.00%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Negatif       0.90      0.69      0.78        26\n",
            "      Netral       0.60      0.50      0.55         6\n",
            "     Positif       0.68      0.94      0.79        18\n",
            "\n",
            "    accuracy                           0.76        50\n",
            "   macro avg       0.73      0.71      0.71        50\n",
            "weighted avg       0.78      0.76      0.76        50\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVes"
      ],
      "metadata": {
        "id": "ef5NVIUmQY5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Changed from MultinomialNB to GaussianNB to handle negative values\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Fungsi untuk memuat file GloVe\n",
        "def load_glove_model(glove_file):\n",
        "    print(\"Loading GloVe model...\")\n",
        "    glove_model = {}\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            if len(values) < 2: # Skip lines that don't have at least a word and one vector component\n",
        "                continue\n",
        "            word = values[0]\n",
        "            try:\n",
        "                vector = np.asarray(values[1:], dtype='float32')\n",
        "                glove_model[word] = vector\n",
        "            except ValueError:\n",
        "                # Optionally, you can print the problematic line for debugging:\n",
        "                # print(f\"Skipping malformed line: {line.strip()}\")\n",
        "                continue # Skip lines where vector conversion fails\n",
        "    print(f\"GloVe model loaded with {len(glove_model)} words.\")\n",
        "    return glove_model\n",
        "\n",
        "# Fungsi untuk mengonversi teks ke vektor menggunakan GloVe\n",
        "def text_to_glove_vector(text, glove_model, embedding_dim=100):\n",
        "    words = text.split()\n",
        "    vectors = []\n",
        "    for word in words:\n",
        "        if word in glove_model:\n",
        "            vectors.append(glove_model[word])\n",
        "    if len(vectors) == 0:  # Jika tidak ada kata yang ditemukan di GloVe\n",
        "        return np.zeros(embedding_dim)\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "# Dataset contoh\n",
        "data = pd.read_csv('Data Manual.csv')\n",
        "\n",
        "# Clean column names by stripping whitespace\n",
        "data.columns = data.columns.str.strip()\n",
        "\n",
        "# Membuat DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Memisahkan data menjadi fitur (X) dan target (y)\n",
        "X = df['Content']\n",
        "y = df['Label']\n",
        "\n",
        "# Membagi data menjadi data latih dan data uji\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Memuat model GloVe (gunakan path file GloVe yang sesuai)\n",
        "# Anda perlu mengunduh file GloVe (misalnya, glove.6B.100d.txt) dan mengunggahnya ke lingkungan Colab Anda\n",
        "# Kemudian ganti 'glove.6B.100d.txt' dengan jalur yang benar ke file tersebut.\n",
        "glove_model = load_glove_model('glove.6B.100d.txt.txt') # Changed filename to match available file\n",
        "\n",
        "# Mengonversi teks ke vektor GloVe\n",
        "X_train_glove = np.array([text_to_glove_vector(text, glove_model) for text in X_train])\n",
        "X_test_glove = np.array([text_to_glove_vector(text, glove_model) for text in X_test])\n",
        "\n",
        "# Melatih model Naive Bayes\n",
        "nb_classifier = GaussianNB() # Changed to GaussianNB\n",
        "nb_classifier.fit(X_train_glove, y_train)\n",
        "\n",
        "# Prediksi dengan data uji\n",
        "y_pred = nb_classifier.predict(X_test_glove)\n",
        "\n",
        "# Evaluasi model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Menampilkan laporan klasifikasi\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMr5c2atQbJh",
        "outputId": "a09714f3-67a8-4507-ec88-3773cab019e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GloVe model...\n",
            "GloVe model loaded with 12264 words.\n",
            "Accuracy: 42.00%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Negatif       0.43      0.12      0.18        26\n",
            "      Netral       0.20      0.33      0.25         6\n",
            "     Positif       0.48      0.89      0.63        18\n",
            "\n",
            "    accuracy                           0.42        50\n",
            "   macro avg       0.37      0.45      0.35        50\n",
            "weighted avg       0.42      0.42      0.35        50\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regresion TF-IDF"
      ],
      "metadata": {
        "id": "tejpGhl6V_5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "# ----- 1. DATASET (contoh sederhana) -----\n",
        "df = pd.read_csv('Data Manual.csv')\n",
        "\n",
        "# Clean column names by stripping whitespace\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df['Content']\n",
        "y = df['Label']\n",
        "\n",
        "# ----- 2. SPLIT DATA -----\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Sastrawi StopWordRemoverFactory to get Indonesian stop words\n",
        "factory = StopWordRemoverFactory()\n",
        "indonesian_stop_words = factory.get_stop_words()\n",
        "\n",
        "# ----- 3. TF-IDF VECTORIZER -----\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1,2),   # unigram + bigram\n",
        "    stop_words=indonesian_stop_words # Use the list of Indonesian stop words\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "# ----- 4. TRAINING LOGISTIC REGRESSION -----\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# ----- 5. PREDIKSI -----\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# ----- 6. HASIL -----\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PXbIj1CWCMv",
        "outputId": "ee0d1365-3792-4ac6-c732-1d20189124a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Negatif       0.90      1.00      0.95        26\n",
            "      Netral       1.00      0.17      0.29         6\n",
            "     Positif       0.80      0.89      0.84        18\n",
            "\n",
            "    accuracy                           0.86        50\n",
            "   macro avg       0.90      0.69      0.69        50\n",
            "weighted avg       0.87      0.86      0.83        50\n",
            "\n",
            "Accuracy: 86.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doc2Vec"
      ],
      "metadata": {
        "id": "x3qhzt2JNZrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhWSivCAKAY5",
        "outputId": "40093ab6-908b-4e2a-88b7-6c4a553929fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Dataset contoh\n",
        "data = pd.read_csv('Data Manual.csv')\n",
        "\n",
        "# Clean column names by stripping whitespace\n",
        "data.columns = data.columns.str.strip()\n",
        "\n",
        "# Membuat DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Memisahkan data menjadi fitur (X) dan target (y)\n",
        "X = df['Content']\n",
        "y = df['Label']\n",
        "\n",
        "# Membagi data menjadi data latih dan data uji\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenisasi dan menandai dokumen\n",
        "def tag_documents(texts):\n",
        "    return [TaggedDocument(words=text.split(), tags=[str(i)]) for i, text in enumerate(texts)]\n",
        "\n",
        "# Menandai dokumen latih dan uji\n",
        "train_documents = tag_documents(X_train)\n",
        "test_documents = tag_documents(X_test)\n",
        "\n",
        "# Melatih model Doc2Vec\n",
        "model = Doc2Vec(vector_size=20, window=2, min_count=1, workers=4, epochs=100)\n",
        "model.build_vocab(train_documents)\n",
        "model.train(train_documents, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "# Mengonversi dokumen ke vektor menggunakan model Doc2Vec\n",
        "X_train_vectors = [model.infer_vector(doc.words) for doc in train_documents]\n",
        "X_test_vectors = [model.infer_vector(doc.words) for doc in test_documents]\n",
        "\n",
        "# Melatih model klasifikasi menggunakan regresi logistik\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train_vectors, y_train)\n",
        "\n",
        "# Prediksi dengan data uji\n",
        "y_pred = classifier.predict(X_test_vectors)\n",
        "\n",
        "# Evaluasi model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-bnsEbONZAj",
        "outputId": "79d84623-324d-414f-c55c-e9836fe8854a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 58.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec"
      ],
      "metadata": {
        "id": "i-b7tLB0emtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim scikit-learn nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbAsGX7ANknn",
        "outputId": "2ea6ed47-c47d-428d-e35e-ba21dd0d1a95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "import pandas as pd # Ensure pandas is imported\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv(\"Data Manual.csv\")\n",
        "data.columns = data.columns.str.strip() # Clean column names\n",
        "\n",
        "# Prepare sentences for Word2Vec training from the 'Content' column\n",
        "sentences = [simple_preprocess(str(text)) for text in data['Content']]\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=sentences,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    sg=1,            # 1 = skip-gram, 0 = CBOW\n",
        "    epochs=20\n",
        ")\n",
        "\n",
        "def doc_vector(doc, model):\n",
        "    words = simple_preprocess(doc)\n",
        "    word_vecs = [model.wv[w] for w in words if w in model.wv]\n",
        "\n",
        "    if len(word_vecs) == 0:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "    return np.mean(word_vecs, axis=0)\n",
        "\n",
        "# Create document vectors X from the 'Content' column\n",
        "X = np.array([doc_vector(str(text), w2v_model) for text in data['Content']])\n",
        "# Create labels y from the 'Label' column\n",
        "y = np.array(data['Label'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "lr_model = LogisticRegression(max_iter=300)\n",
        "lr_model.fit(X_train, y_train)\n",
        "y_pred = lr_model.predict(X_test)\n",
        "\n",
        "print(\"Akurasi:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LGH6FMie20p",
        "outputId": "b837a31e-1814-4066-c810-7e16c60cffbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi: 0.44\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Negatif       0.50      0.42      0.46        26\n",
            "      Netral       0.00      0.00      0.00         6\n",
            "     Positif       0.39      0.61      0.48        18\n",
            "\n",
            "    accuracy                           0.44        50\n",
            "   macro avg       0.30      0.34      0.31        50\n",
            "weighted avg       0.40      0.44      0.41        50\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DECISION TREE"
      ],
      "metadata": {
        "id": "7ba6znnXFxP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec"
      ],
      "metadata": {
        "id": "AlgJkympF60_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLM1gGS-HHtG",
        "outputId": "9e69c3a5-c7f7-438d-d842-edcf007c90e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Sastrawi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cW_oIdwOKZE7",
        "outputId": "524ba2ec-4641-4a05-a577-0327d91a9eb3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 1. Import Library\n",
        "# ============================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# ============================\n",
        "# 2. Dataset Contoh\n",
        "# ============================\n",
        "documents = pd.read_csv('DataManual.csv')\n",
        "documents.columns = documents.columns.str.strip()\n",
        "\n",
        "# ============================\n",
        "# 3. Preprocessing (tokenisasi)\n",
        "# ============================\n",
        "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents['Content']]\n",
        "\n",
        "# ============================\n",
        "# 4. Training Word2Vec\n",
        "# ============================\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=tokenized_docs,\n",
        "    vector_size=100,  # Dimensi embedding\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    workers=4\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# 5. Fungsi membuat fitur rata-rata Word2Vec\n",
        "# ============================\n",
        "def document_vector(doc):\n",
        "    # Filter kata yang ada di vocabulary Word2Vec\n",
        "    doc = [word for word in doc if word in w2v_model.wv]\n",
        "    # Jika tidak ada kata yang dikenal, kembalikan vektor nol\n",
        "    if len(doc) == 0:\n",
        "        return np.zeros(w2v_model.vector_size)\n",
        "    # Hitung rata-rata embedding\n",
        "    return np.mean(w2v_model.wv[doc], axis=0)\n",
        "\n",
        "# Buat fitur untuk semua dokumen\n",
        "X = np.array([document_vector(doc) for doc in tokenized_docs])\n",
        "y = np.array(documents['Label'])\n",
        "\n",
        "# ============================\n",
        "# 6. Split dataset\n",
        "# ============================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# 7. Train Decision Tree\n",
        "# ============================\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# ============================\n",
        "# 8. Evaluasi Model\n",
        "# ============================\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"Akurasi:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoxCd28-F_42",
        "outputId": "f8486c7c-8e6f-4510-c040-cd1870a6e021"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi: 0.56\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Negatif       0.59      0.56      0.58        34\n",
            "      Netral       0.17      0.14      0.15        14\n",
            "     Positif       0.68      0.78      0.72        27\n",
            "\n",
            "    accuracy                           0.56        75\n",
            "   macro avg       0.48      0.49      0.48        75\n",
            "weighted avg       0.54      0.56      0.55        75\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF"
      ],
      "metadata": {
        "id": "i94EAkkdLY3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features (X) and target (y)\n",
        "X = documents['Content']\n",
        "y = documents['Label']\n",
        "\n",
        "# ----- 2. SPLIT DATA -----\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Sastrawi StopWordRemoverFactory to get Indonesian stop words\n",
        "factory = StopWordRemoverFactory()\n",
        "indonesian_stop_words = factory.get_stop_words()\n",
        "\n",
        "# ----- 3. TF-IDF VECTORIZER -----\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1,2),   # unigram + bigram\n",
        "    stop_words=indonesian_stop_words # Use the list of Indonesian stop words\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "# ============================\n",
        "# 7. Train Decision Tree\n",
        "# ============================\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train_tfidf, y_train) # Corrected to use TF-IDF features\n",
        "\n",
        "# ----- 5. PREDIKSI -----\n",
        "y_pred = clf.predict(X_test_tfidf) # Corrected to use clf and TF-IDF features\n",
        "\n",
        "# ----- 6. HASIL -----\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr8Bsd4dHGIY",
        "outputId": "d4e74810-0299-4221-9fc7-b8970de313b7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Negatif       0.88      0.88      0.88        26\n",
            "      Netral       0.43      0.50      0.46         6\n",
            "     Positif       0.88      0.83      0.86        18\n",
            "\n",
            "    accuracy                           0.82        50\n",
            "   macro avg       0.73      0.74      0.73        50\n",
            "weighted avg       0.83      0.82      0.82        50\n",
            "\n",
            "Accuracy: 82.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM (Linear)"
      ],
      "metadata": {
        "id": "4OmJWbP9MH_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec"
      ],
      "metadata": {
        "id": "xOQK0knyMRux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================\n",
        "# 1. IMPORT LIBRARY\n",
        "# =======================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# =======================================\n",
        "# 2. LOAD DATA CSV\n",
        "# =======================================\n",
        "# Pastikan file manual.csv memiliki kolom: \"text\", \"label\"\n",
        "df = pd.read_csv(\"DataManual.csv\")\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "texts = df[\"Content\"].astype(str).tolist()\n",
        "labels = df[\"Label\"].astype(str).tolist()\n",
        "\n",
        "\n",
        "# =======================================\n",
        "# 3. TOKENISASI\n",
        "# =======================================\n",
        "tokenized_docs = [word_tokenize(t.lower()) for t in texts]\n",
        "\n",
        "\n",
        "# =======================================\n",
        "# 4. TRAIN WORD2VEC\n",
        "# =======================================\n",
        "w2v_dim = 100\n",
        "\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=tokenized_docs,\n",
        "    vector_size=w2v_dim,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    workers=4\n",
        ")\n",
        "\n",
        "\n",
        "# =======================================\n",
        "# 5. FUNGSI MEMBUAT VEKTOR WORD2VEC (AVERAGE EMBEDDING)\n",
        "# =======================================\n",
        "def document_vector(doc):\n",
        "    words = [w for w in doc if w in w2v_model.wv]\n",
        "    if len(words) == 0:\n",
        "        return np.zeros(w2v_dim)\n",
        "    return np.mean(w2v_model.wv[words], axis=0)\n",
        "\n",
        "\n",
        "# Build fitur untuk semua dokumen\n",
        "X = np.array([document_vector(doc) for doc in tokenized_docs])\n",
        "y = np.array(labels)\n",
        "\n",
        "\n",
        "# =======================================\n",
        "# 6. NORMALISASI FITUR (PENTING UNTUK SVM)\n",
        "# =======================================\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "# =======================================\n",
        "# 7. SPLIT DATA\n",
        "# =======================================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# =======================================\n",
        "# 8. TRAIN MODEL SVM\n",
        "# =======================================\n",
        "svm_model = SVC(kernel=\"linear\", C=1)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# =======================================\n",
        "# 9. EVALUASI\n",
        "# =======================================\n",
        "y_pred = svm_model.predict(X_test)\n",
        "print(\"Akurasi:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_3P8yduMT_C",
        "outputId": "15ac7ee0-4d9b-44be-a57b-9d555bb994ae"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi: 0.74\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Negatif       0.94      0.62      0.74        26\n",
            "      Netral       0.36      0.83      0.50         6\n",
            "     Positif       0.84      0.89      0.86        18\n",
            "\n",
            "    accuracy                           0.74        50\n",
            "   macro avg       0.71      0.78      0.70        50\n",
            "weighted avg       0.84      0.74      0.76        50\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer # Added import\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd # Added import\n",
        "\n",
        "# =======================================\n",
        "# 2. LOAD DATA CSV (re-include for self-containment)\n",
        "# =======================================\n",
        "df = pd.read_csv(\"DataManual.csv\")\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "texts = df[\"Content\"].astype(str).tolist()\n",
        "labels = df[\"Label\"].astype(str).tolist()\n",
        "\n",
        "# =======================================\n",
        "# 3. BAG OF WORDS (COUNT VECTORIZER)\n",
        "# =======================================\n",
        "vectorizer = CountVectorizer()\n",
        "X_bow = vectorizer.fit_transform(texts).toarray()\n",
        "\n",
        "\n",
        "# =======================================\n",
        "# 4. NORMALISASI (OPSIONAL, TAPI BAIK UNTUK SVM)\n",
        "# =======================================\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_scaled = scaler.fit_transform(X_bow)\n",
        "\n",
        "\n",
        "# =======================================\n",
        "# 5. SPLIT DATA\n",
        "# =======================================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# =======================================\n",
        "# 6. TRAIN SVM\n",
        "# =======================================\n",
        "svm_model = SVC(kernel=\"linear\", C=1)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# =======================================\n",
        "# 7. EVALUASI\n",
        "# =======================================\n",
        "y_pred = svm_model.predict(X_test)\n",
        "print(\"Akurasi:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86jmV8zzNT7I",
        "outputId": "45647e4d-9519-4ec0-eace-7e6a0c621341"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi: 0.84\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Negatif       0.96      0.88      0.92        26\n",
            "      Netral       0.40      0.33      0.36         6\n",
            "     Positif       0.81      0.94      0.87        18\n",
            "\n",
            "    accuracy                           0.84        50\n",
            "   macro avg       0.72      0.72      0.72        50\n",
            "weighted avg       0.84      0.84      0.84        50\n",
            "\n"
          ]
        }
      ]
    }
  ]
}